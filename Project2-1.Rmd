---
title: "Project 2 - Dataset 1"
author: "Gillian McGovern"
date: "2025-03-05"
output: html_document
---

## Dataset 1 - "Women of the Year in 2020"

## Overview

For the first untidy data set, I chose to grab the table from https://www.bbc.com/news/world-55042935 which showcases BBC's women of the year in 2020. I chose this data set because I thought it would be a fun challenge to try to extract information via reading the html page directly. Since the data would be coming from an HTML page, the data would be present in a unique format.

This data set is untidy because there really isn't any structure at all as it is coming from an HTML page. For this you need to create an observation for each women, with variables representing the different categories of information BBC provides, such as location, career, etc.

Source: https://github.com/rfordatascience/tidytuesday/tree/2590ecee108c17b5ab5b3e02dc7ba6e25bf3f081/data/2020/2020-12-08

## Load the Libraries

Let's first load the libraries:

```{r}
library(tidyverse)
library(rvest)
library(stringi)
library(tidyr)
```

## Read the Data

For this particular set of data, I wanted to read from the page directly, as opposed to copying and pasting all the information into a csv. To do this, I used `read_html`.

I then wanted to grab the information from the table. My goal was to get the elements from the div id `fw-100-women-2019` or `card__header` since that's where each "card" is located on the website, but I was not able to get an element/html node that worked (using `html_elements` and `html_nodes`). Since web scraping wasn't the point of this project, I decided to not spend too much time debugging, and just grabbed the whole body using `html_elements`.

Then I know each card can be found in `cardId` within the body, so I used that to search for all the observations that will eventually need to go into my dataframe:

```{r}
html <- read_html("https://www.bbc.com/news/world-55042935")

# Grab the html contents
body <- html_elements(html, css = "body")

# Retrieve the text
final_string <- body |> html_text(trim = TRUE)

# Clean up
string_replaced <- gsub('\\"cardId', "cardId", final_string, fixed=TRUE)

# Each woman is within a "cardId"
cardIds <- str_extract(string_replaced, '([{]cardId.*[}])')

# Break up the unique cardIds
main_data <- str_split(cardIds, "\\},\\{cardId")
glimpse(main_data)
```

Since we now have a list, let's insert the list into a data frame:

```{r}
final_main_data <- main_data[[1]]
df <- data.frame(
    id = c(1:100),
    info = final_main_data
)
df
```

## Clean Up and Tidying

We now have a very messy untidy data frame. It is untidy since there's no real structure to this data frame. All the information is in one generic column called "info". Each cell doesn't represent a single value.

Let's first separate `info` into proper variables:

```{r}
df2 <- df %>%
  separate(info, c(NA, 'first_name_info'), sep=",\\\\", remove = FALSE) |>
  separate(first_name_info, c(NA, 'first_name'), sep="firstName\\\\", remove = FALSE) |>
  separate(info, c(NA, 'second_name_info'), sep="secondName\\\\", remove = FALSE) |>
  separate(second_name_info, c('second_name', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'image_url_info'), sep="imageUrl\\\\", remove = FALSE) |>
  separate(image_url_info, c('image_url', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'location_info'), sep="location\\\\", remove = FALSE) |>
  separate(location_info, c('location', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'twitter_info'), sep="twitter\\\\", remove = FALSE) |>
  separate(twitter_info, c('twitter', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'instagram_info'), sep="instagram\\\\", remove = FALSE) |>
  separate(instagram_info, c('instagram', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'role_info'), sep="role\\\\", remove = FALSE) |>
  separate(role_info, c('role', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'paragraphs_info'), sep="paragraphs\\\\", remove = FALSE) |>
  separate(paragraphs_info, c('paragraphs', NA), sep=",\\\\", remove = FALSE) |>
  separate(info, c(NA, 'category_info'), sep="category\\\\", remove = FALSE) |>
  separate(category_info, c('category', NA), sep=",\\\\", remove = FALSE)
glimpse(df2)
```

To make things easier, let's clean up some of the data:

```{r}
# Clean up the data
df2$first_name<- str_sub(df2$first_name,5,-3)
df2$second_name<- str_sub(df2$second_name,5,-3)
df2$image_url<- str_sub(df2$image_url,5,-3)
df2$location<- str_sub(df2$location,5,-3)
df2$twitter<- str_sub(df2$twitter,5,-3)
df2$instagram<- str_sub(df2$instagram,5,-3)
df2$role<- str_sub(df2$role,5,-3)
df2$paragraphs<- str_sub(df2$paragraphs,5,-3)
df2$category<- str_sub(df2$category,5,-3)
df2$paragraphs <-str_sub(df2$paragraphs,44,-16)
df2$paragraphs <- str_replace_all(df2$paragraphs, c("(\\\\u003c)"="","(\\\\u003e)"="","(/pp)"=" ","(/p)"="", "(\\\\\\\\n)"=""))
```

We have way too many variables that represent the same exact information. Let's now remove so the data frame is tidy (each cell contains a single value):

```{r}
# Remove columns we won't need for this data frame to make things simpler
cols_to_delete <- c("info", "first_name_info","second_name_info", "image_url_info", "location_info", "twitter_info", "instagram_info", "role_info", "paragraphs_info", "category_info")
df3 <- df2[ , !(names(df2) %in% cols_to_delete)]
df3$category<-str_replace_all(df3$category, c('\\\\\\\"'=""))
glimpse(df3)
```

We now have a tidy data frame where each row is an observation representing a woman, each descriptive info category from the HTML page is now a variable, and each cell contains a single value.

## Analysis

Let's first look into where these women are from:

```{r}
# Location
location <- table(df3$location)
location_df <- as.data.frame(location)
location_df_sorted <-location_df |>
  arrange(desc(Freq))
location_df_sorted

# Plot the data
location_df_top_15 <- location_df_sorted[1:15,]
location_df_top_15
ggplot(location_df_top_15, aes(x=Var1, y=Freq)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

As you can see, the US leads with the most women, followed by the UK and then India and Thailand.

Now let's look at the type of careers these women have (top 10 roles):

```{r}
# Role
role <- table(df3$role)
role_df <- as.data.frame(role)
role_df_sorted <-role_df |>
  arrange(desc(Freq))
role_df_sorted
# Plot the data
role_df_sorted_top_10 <- role_df_sorted[1:10,]
ggplot(role_df_sorted_top_10, aes(x=Var1, y=Freq)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

As you can see the most frequent occupation is an Activist, followed by a Campaigner, then a Disability activist. Since these women are most likely trying to make a difference in the world, which is how BBC recognizes them, it makes sense that a good portion of these women are activists.

Let's see how many roles mention `Activist` or `activist`:

```{r}
df3_activist <- df3 %>%
  filter(str_detect(role, 'Activist|activist'))
df3_activist
```

As you can see, there are 24 women that are some sort of activist, which is almost a quarter of the women.

Now let's see what the general category variable shows us:

```{r}
# Category
category <- table(df3$category)
category_df <- as.data.frame(category)
category_df_sorted <-category_df |>
  arrange(desc(Freq))
category_df_sorted
# Plot the data
category_df_sorted_top_10 <- category_df_sorted[1:10,]
ggplot(category_df_sorted_top_10, aes(x=Var1, y=Freq)) +
    geom_bar(stat='identity', position='dodge')
```

As you can see Knowledge and Leadership are the main categories, which is again, not too surprising.

Now let's see for the top locations, which categories are being represented:

```{r}
locations <- unique(location_df_top_15$Var1)
cat_loc <- with(df3, table(df3$location, df3$category))
cat_loc_df <- as.data.frame(cat_loc)
cat_loc_df <- cat_loc_df |>
  filter(Var1 %in% locations)

# Combine location and category
ggplot(cat_loc_df, aes(x=Var1, y=Freq, fill=Var2)) +
  geom_bar(stat='identity', position='dodge') +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

What's interesting is that the US has the most for `Identity` while the UK has the most for `Knowledge`. We also know that the US and the UK are generally leading the overall women of the year.

## Conclusions

I think this type of data set was a good learning experience for reading html from a page. I believe my code could use a lot of improvement, but since web scraping is not the point of this project, I chose to not look into it further.

This data felt like a good real world example of how data can be presented. Originally, each cell in the data frame contained all the information within the data frame, so making the values into variables was essential to making the data frame tidy.

